
if (!require(ggplot2)){ 
        install.packages('ggplot2') 
}
if (!require(e1071)){ 
        install.packages('e1071')
}

if (!require(magrittr)){ 
        install.packages('magrittr') 
}

if (!require(dplyr)){ 
        install.packages('dplyr') 
}

if (!require(RGtk2)){ 
        install.packages('RGtk2') 
}

if (!require(rattle)){ 
        install.packages('rattle') 
}

if (!require(rpart.plot)){ 
        install.packages('rpart.plot') 
}

if (!require(rpart)){ 
        install.packages('rpart') 
}

if (!require(party)){ 
        install.packages('party') 
}

if (!require(partykit)){ 
        install.packages('partykit') 
}

if (!require(ROSE)){ 
        install.packages('ROSE') 
}

if (!require(ROCR)){ 
        install.packages('ROCR') 
}

if (!require(caret)){ 
        install.packages('caret') 
}

if (!require(xgboost)){ 
        install.packages('xgboost') 
}

if (!require(nnet)){ 
        install.packages('nnet') 
}

if (!require(ineq)){ 
        install.packages('ineq') 
}

if (!require(neuralnet)){ 
        install.packages('neuralnet') 
}

if (!require(Rtsne)){ 
        install.packages('Rtsne') 
}

if (!require(rCharts)){ 
        install.packages('rCharts') 
}

if (!require(rsconnect)){ 
        install.packages('rsconnect') 
}

if (!require(shiny)){ 
        install.packages('shiny') 
}

if (!require(shinydashboard)){ 
        install.packages('shinydashboard') 
}

library(magrittr)
library(dplyr)

library(gridExtra)
library(ggplot2)

library(RGtk2)
library(rattle)
library(rpart.plot)
library(rpart)
library(e1071)
library(party)
library(partykit)
library(ROSE)

library(ROCR)

library(caret)
library(xgboost)
library(nnet)
library(ineq)
library(neuralnet)
library(Rtsne)
library(rCharts)
library(rsconnect)
library(shinydashboard)



cardiac = read.csv("http://www.stat.ucla.edu/projects/datasets/cardiac.dat")
head(cardiac)

nrow(cardiac)

ncol(cardiac)

##Since the last 10 variables were generated by the researchers we will go ahead and ignore them. 
remove = c("phat", "event...", "mics", "deltaEF", "newpkmphr", "gdpkmphr", "gdmaxmphr", "gddpeakdp", "gdmaxdp", "hardness")
cardiac_new = cardiac[, !(colnames(cardiac) %in% remove)]
ncol(cardiac_new)

smp_size = floor(0.75 * nrow(cardiac))
train_ind = sample(seq_len(nrow(cardiac)), size = smp_size)
train = cardiac[train_ind,]
test = cardiac[-train_ind,]



train %>% select(newMI) %>% ggplot(aes(x=factor(x=newMI, labels = c("YES","NO")))) + geom_bar() + xlab('Remodeled') + theme_light()
dev.new()
train %>% select(newPTCA) %>% ggplot(aes(x=factor(x= newPTCA, labels = c("YES","NO")))) + geom_bar() + xlab('Remodeled') + theme_light()
dev.new()

train %>% select(newCABG) %>% ggplot(aes(x=factor(x= newCABG, labels = c("YES","NO")))) + geom_bar() + xlab('Remodeled') + theme_light()
dev.new()
train %>% select(death) %>% ggplot(aes(x=factor(x= death, labels = c("YES","NO")))) + geom_bar() + xlab('Remodeled') + theme_light()
dev.new()


##Clearly, our data is not normally distributed. Another way to view this is by using the table function
table(train$cls)

table(train$newMI)

table(train$newPTCA)

table(train$newCABG)

table(train$death)

table(train$any.event)

prop.table(table(train$newMI))

prop.table(table(train$newPTCA))

prop.table(table(train$newCABG))

prop.table(table(train$death))

prop.table(table(train$any.event))
 
##As we can see the newMI dependent variable contains 94.7% No and 5.3% of Yes
##The newPTCA dependent variable contains 95.7% No and 4.3% of Yes
##The newCAGB dependent variable contains 94.0% No and 6.0% of Yes
##The death dependent variable contains 96.4% No and 3.6% of Yes
##The any.event dependent variable, which is a combination of all four dependent variables, contains 85.4% No and 14.6% of Yes. 

##As a result of this we will need to transform the imbalanced data to a balanced one.
##The following approaches will be used and the best chosen. 1)Undersampling 2)Oversampling 3)Synthetic Data Generation 4)Cost Sensitive Learning


plotHist = function(data_in, i) {
        data = data.frame(x = data_in[[i]])
        p = ggplot(data=data, aes(x=factor(x))) + stat_count() + xlab(colnames(data_in)[i]) + theme_light() + 
        theme(axis.text.x = element_text(angle = 90, hjust = 1))
        return (p)
        }

plotDen = function(data_in, i){
          data = data.frame(x = data_in[[i]])
          p = ggplot(data= data) + geom_line(aes(x = x), stat = 'density', size = 1,alpha = 1.0) +
          xlab(paste0((colnames(data_in)[i]), '\n', 'Skewness: ',round(skewness(data_in[[i]], na.rm = TRUE), 2))) + theme_light() 
          return(p)
          }


doPlots = function(data_in, fun, ii, ncol=3) {
              pp = list()
              for (i in ii) {
              p = fun(data_in=data_in, i=i)
              pp = c(pp, list(p))
                   }
          do.call("grid.arrange", c(pp, ncol=ncol))
          }


car_var = c("gender", "chestpainposECG", "equivecg", "restwma", "posSE", "hxofHT", "hxofdm", "hxofcig", "hxofMI", "hxofPTCA", "hxofCABG")
num_var = c("bhr", "basebp", "basedp", "pkhr", "sbp", "dp", "dose", "maxhr", "X.mphr.b.", "mbp", "dpmaxdo", "dobdose", "age", "baseEF", "dobEF")

train_car = train[,colnames(train) %in% car_var]
train_num = train[,colnames(train) %in% num_var]
names(train_car)
doPlots(train_car, fun = plotHist, ii = 1:10, ncol = 3)


##Visualisation using t-sne (t distributed stochastic neighbor embedding)
tsne_newMI = Rtsne(train[,-c(22,23,24,25,32)], dims = 2, perplexity = 30, verbose = TRUE, max_iter = 500)

colors_newMI = rainbow(length(unique(as.factor(train$newMI))))
names(colors_newMI) = unique(as.factor(train$newMI))
dev.new()
plot(tsne_newMI$Y, t = 'n', main = "tsne_newMI")
text(tsne_newMI$Y, label = as.factor(train$newMI), col = colors_newMI[as.factor(train$newMI)])

##newPTCA
tsne_newPTCA = Rtsne(train[,-c(22,23,24,25,32)], dims = 2, perplexity = 30, verbose = TRUE, max_iter = 500)

colors_newPTCA = rainbow(length(unique(as.factor(train$newPTCA))))
names(colors_newPTCA) = unique(as.factor(train$newPTCA))
dev.new()
plot(tsne_newPTCA$Y, t = 'n', main = "tsne_newPTCA")
text(tsne_newPTCA$Y, label = as.factor(train$newPTCA), col = colors_newPTCA[as.factor(train$newPTCA)])


##newCABG
tsne_newCABG = Rtsne(train[,-c(22,23,24,25,32)], dims = 2, perplexity = 30, verbose = TRUE, max_iter = 500)

colors_newCABG = rainbow(length(unique(as.factor(train$newCABG))))
names(colors_newCABG) = unique(as.factor(train$newCABG))
dev.new()
plot(tsne_newCABG$Y, t = 'n', main = "tsne_newCABG")
text(tsne_newCABG$Y, label = as.factor(train$newCABG), col = colors_newCABG[as.factor(train$newCABG)])


##death
tsne_death = Rtsne(train[,-c(22,23,24,25,32)], dims = 2, perplexity = 30, verbose = TRUE, max_iter = 500)

colors_death = rainbow(length(unique(as.factor(train$death))))
names(colors_death) = unique(as.factor(train$death))
dev.new()
plot(tsne_death$Y, t = 'n', main = "tsne_death")
text(tsne_death$Y, label = as.factor(train$death), col = colors_death[as.factor(train$death)])



##Classification model

## earlier we talked about how highly unbalanced our data is. Next, we will go ahead and balance our data. but before, lets see how unbalanced our response variable is again
table(train$newMI)
table(train$newMI)
table(train$newPTCA)
table(train$newCABG)
table(train$death)

##Four balancing techniques can be employed here
##The over sampling technique
##The under sampling technique
##A combination of both over sampling and under sampling
##Lastly, the ROSE technique
##The ROSE technique is said to be the best so we will go ahead and work with it and one of the other three for comparism purposes.

##CLASIFICATION TREES

##CLASSIFICATION OF THE NEWMI 
data_bal_over_newMI = ovun.sample(newMI ~ . - newPTCA - newCABG - death - any.event, data = train, method = "over", N = 792)$data
table(data_bal_over_newMI$newMI)

data_bal_rose_newMI = ROSE(newMI ~ . - newPTCA - newCABG - death - any.event, data = train)$data
table(data_bal_rose_newMI$newMI)

###Building a decision tree with both sample sizes
tree_over = rpart(newMI ~ . - newPTCA - newCABG - death - any.event, data = data_bal_over_newMI)
tree_rose = rpart(newMI ~ . - newPTCA - newCABG - death - any.event, data = data_bal_rose_newMI)

##Plot the outputs
dev.new()
fancyRpartPlot(tree_over)
dev.new()
fancyRpartPlot(tree_rose)

##Prediction
pred_over = predict(tree_over, newdata = test)
pred_rose = predict(tree_rose, newdata = test)

##Evaluate the accuracy of both resampling techniques using the inbuilt roc.curve function to obtain AUC
auc_curve_over = roc.curve(test$newMI, pred_over)
auc_curve_over
##Area under the curve (AUC): 0.966
auc_curve_rose = roc.curve(test$newMI, pred_rose)
auc_curve_rose
##Area under the curve (AUC): 0.974

##Clearly, ROSE resampling technique performed better


##CLASSIFICATION OF THE NEWPTCA 
data_bal_over_newPTCA = ovun.sample(newPTCA ~ . - newMI - newCABG - death - any.event, data = train, method = "over", N = 792)$data
table(data_bal_over_newPTCA$newPTCA)

data_bal_rose_newPTCA = ROSE(newPTCA ~ . - newMI - newCABG - death - any.event, data = train)$data
table(data_bal_rose_newPTCA$newPTCA)

###Building a decision tree with both sample sizes
tree_over_newPTCA = rpart(newPTCA ~ . - newMI - newCABG - death - any.event, data = data_bal_over_newPTCA)
tree_rose_newPTCA = rpart(newPTCA ~ . - newMI - newCABG - death - any.event, data = data_bal_rose_newPTCA)

##Plot the outputs
dev.new()
fancyRpartPlot(tree_over_newPTCA)
dev.new()
fancyRpartPlot(tree_rose_newPTCA)

##Prediction
pred_over_newPTCA = predict(tree_over_newPTCA, newdata = test)
pred_rose_newPTCA = predict(tree_rose_newPTCA, newdata = test)

##Evaluate the accuracy of both resampling techniques using the inbuilt roc.curve function to obtain AUC
auc_curve_over_newPTCA = roc.curve(test$newPTCA, pred_over_newPTCA)
auc_curve_over_newPTCA
##Area under the curve (AUC): 0.592

auc_curve_rose_newPTCA = roc.curve(test$newPTCA, pred_rose_newPTCA)
auc_curve_rose_newPTCA
##Area under the curve (AUC): 0.927

##Clearly, ROSE resampling technique performed better




##CLASSIFICATION OF THE NEWCABG 
data_bal_over_newCABG = ovun.sample(newCABG ~ . - newMI - newPTCA - death - any.event, data = train, method = "over", N = 792)$data
table(data_bal_over_newCABG$newCABG)

data_bal_rose_newCABG = ROSE(newCABG ~ . - newMI - newPTCA - death - any.event, data = train)$data
table(data_bal_rose_newCABG$newCABG)

###Building a decision tree with both sample sizes
tree_over_newCABG = rpart(newCABG ~ . - newMI - newPTCA - death - any.event, data = data_bal_over_newCABG)
tree_rose_newCABG = rpart(newCABG ~ . - newMI - newPTCA - death - any.event, data = data_bal_rose_newCABG)

##Plot the outputs
dev.new()
fancyRpartPlot(tree_over_newCABG)
dev.new()
fancyRpartPlot(tree_rose_newCABG)

##Prediction
pred_over_newCABG = predict(tree_over_newCABG, newdata = test)
pred_rose_newCABG = predict(tree_rose_newCABG, newdata = test)

##Evaluate the accuracy of both resampling techniques using the inbuilt roc.curve function to obtain AUC
auc_curve_over_newCABG = roc.curve(test$newCABG, pred_over_newCABG)
auc_curve_over_newCABG
##Area under the curve (AUC): 0.898

auc_curve_rose_newCABG = roc.curve(test$newCABG, pred_rose_newCABG)
auc_curve_rose_newCABG
##Area under the curve (AUC): 0.924

##Clearly, ROSE resampling technique performed better




##CLASSIFICATION OF THE DEATH VARIABLE 
data_bal_over_death = ovun.sample(death ~ . - newMI - newPTCA - newCABG - any.event, data = train, method = "over", N = 792)$data
table(data_bal_over_death$death)

data_bal_rose_death = ROSE(death ~ . - newMI - newPTCA - newCABG - any.event, data = train)$data
table(data_bal_rose_death$death)

###Building a decision tree with both sample sizes
tree_over_death = rpart(death ~ . - newMI - newPTCA - newCABG - any.event, data = data_bal_over_newCABG)
tree_rose_death = rpart(death ~ . - newMI - newPTCA - newCABG - any.event, data = data_bal_rose_newCABG)

##Plot the outputs
dev.new()
fancyRpartPlot(tree_over_death)
dev.new()
fancyRpartPlot(tree_rose_death)

##Prediction
pred_over_death = predict(tree_over_death, newdata = test)
pred_rose_death = predict(tree_rose_death, newdata = test)

##Evaluate the accuracy of both resampling techniques using the inbuilt roc.curve function to obtain AUC
auc_curve_over_death = roc.curve(test$death, pred_over_death)
auc_curve_over_death
##Area under the curve (AUC): 0.997

auc_curve_rose_death = roc.curve(test$death, pred_rose_death)
auc_curve_rose_death
##Area under the curve (AUC): 0.986

##Clearly, ROSE resampling technique performed better







##SUPPORT VECTOR MACHINE
svm = read.csv("http://www.stat.ucla.edu/projects/datasets/cardiac.dat")
head(svm)
svm_mat = as.matrix(svm)

##Due to the unbalance nature of our independent variable, we will have to balance the data using ROSE package

##Next, we dichotomize our data to train and test data set. 
svm_smp_size = floor(0.75 * nrow(svm))
svm_train_ind = sample(seq_len(nrow(svm)), size = svm_smp_size)
svm_train = svm[svm_train_ind,]
svm_test = svm[-svm_train_ind,]


##CLASSIFICATION USING SVM ON newMI
##balancing the dependent variable
svm_train_rose_newMI = ROSE(newMI ~ . -death - newPTCA - newCABG - any.event, data = svm_train)$data
table(svm_train_rose_newMI$newMI)


svm_validate_newMI <- svm_train_rose_newMI[sample(nrow(svm_train_rose_newMI),size=20000, replace=TRUE),]
svm_develop_newMI <- svm_train_rose_newMI[sample(nrow(svm_train_rose_newMI),size=20000, replace=TRUE),]

table(svm_validate_newMI$newMI)/length(svm_validate_newMI$newMI)

tmp_svm_newMI = svm(as.factor(newMI) ~ . - newPTCA - newCABG - death - any.event, data = svm_validate_newMI)
summary(tmp_svm_newMI)


##prediction trained model
Predicted_y_newMI <- predict(tmp_svm_newMI, svm_test)
table_svm_newMI <- table(pred = Predicted_y_newMI, true = svm_test$newMI)/length(svm_test$newMI)
table_svm_newMI

##We apply a function that will allow us get different kernel functions, gamma and cost values, and ultimately get the get combiunation of values for gamma and cost
##The computation cost of this seems high but it does the job well
svm_simulate_newMI  = function(){
  gamma_value_newMI  = c(10^seq(-5,-1, by=1))
  cost_value_newMI = c(10^seq(-3,1, by=1))
  svm_df_newMI = data.frame()
    for(g in gamma_value_newMI){
      for(c in cost_value_newMI){
        # train model
        tmp_svm_in_iter_newMI = svm(as.factor(newMI) ~ . - newPTCA - newCABG - death - any.event,
                         kernel="radial",
                         gamma=g,
                         cost=c,
                         data = svm_validate_newMI)
        # validate model
        Predicted_y_in_iter_newMI <- predict(tmp_svm_in_iter_newMI, svm_test)
        # Compare Observed and Predicted
        table_svm_newMI <- prop.table(table(pred = Predicted_y_in_iter_newMI,
                    true = svm_test$newMI)/length(svm_test$newMI),1)
        in_df_newMI <- data.frame(gamma=g,
                      cost=c,
                      accuracy=table_svm_newMI[4] )
        svm_df_newMI <- rbind(svm_df_newMI,in_df_newMI)
      }
    }
  svm_df_newMI
}
svm_simulate_out_newMI = svm_simulate_newMI()
svm_simulate_out_newMI
svm_simulate_out_newMI_noNons = svm_simulate_out_newMI[complete.cases(svm_simulate_out_newMI),]

g_newMI = tail(filter(svm_simulate_out_newMI, accuracy == apply(svm_simulate_out_newMI_noNons, 2, max)[3])[1], 1)
c_newMI = tail(filter(svm_simulate_out_newMI, accuracy == apply(svm_simulate_out_newMI_noNons, 2, max)[3])[2], 1)

tmp_svm_proper_newMI = svm(as.factor(newMI) ~ . - newPTCA - newCABG - death - any.event, kernel="radial", gamma=g_newMI, cost=c_newMI, data=svm_develop_newMI)

Predicted_y_proper_newMI = predict(tmp_svm_proper_newMI, svm_test)


confusionMatrix(data = factor(Predicted_y_proper_newMI), reference = factor(svm_test$newMI))

##ROC Curve
##Not working properly
#svm_ypredscore_newMI = predict(tmp_svm_proper_newMI, svm_test, type = "decision")

#svm_perf_obj_newMI = prediction(as.numeric(svm_ypredscore_newMI), svm_test[,22])

#svm_perf_newMI = performance(svm_perf_obj_newMI, measure = "tpr", x.measure = "fpr")
#dev.new()
#plot(svm_perf_newMI,
     #main="Cross-Sell - ROC Curves for newMI",
     #xlab="1 – Specificity: False Positive Rate",
     #ylab="Sensitivity: True Positive Rate",
     #col="blue")
#abline(0,1,col="grey")


##AUC 
svm_auc_newMI = roc.curve(svm_test$newMI, Predicted_y_proper_newMI)
svm_auc_newMI


##Using Gini Index and Lorenz curve to view the inequality among the population of our data
svm_ypredscore_newMI = predict(tmp_svm_proper_newMI, svm_test, type = "decision")
svm_gini_index_newMI = ineq(as.numeric(svm_ypredscore_newMI), type = "Gini")
svm_gini_index_newMI

dev.new()
plot(Lc(as.numeric(svm_ypredscore_newMI)), col = "darkred", lwd = 2)










##CLASSIFICATION USING SVM ON newPTCA
##balancing the dependent variable
svm_train_rose_newPTCA = ROSE(newPTCA ~ . -death - newMI - newCABG - any.event, data = svm_train)$data
table(svm_train_rose_newPTCA$newPTCA)

svm_validate_newPTCA <- svm_train_rose_newPTCA[sample(nrow(svm_train_rose_newPTCA),size=20000, replace=TRUE),]
svm_develop_newPTCA <- svm_train_rose_newPTCA[sample(nrow(svm_train_rose_newPTCA),size=20000, replace=TRUE),]

table(svm_validate_newPTCA$newPTCA)/length(svm_validate_newPTCA$newPTCA)

tmp_svm_newPTCA = svm(as.factor(newPTCA) ~ . - newMI - newCABG - death - any.event, data = svm_validate_newPTCA)
summary(tmp_svm_newPTCA)


##prediction trained model
Predicted_y_newPTCA <- predict(tmp_svm_newPTCA, svm_test)
table_svm_newPTCA <- table(pred = Predicted_y_newPTCA, true = svm_test$newPTCA)/length(svm_test$newPTCA)
table_svm_newPTCA

##We apply a function that will allow us get different kernel functions, gamma and cost values, and ultimately get the get combiunation of values for gamma and cost
##The computation cost of this seems high but it does the job well
svm_simulate_newPTCA  = function(){
  gamma_value_newPTCA  = c(10^seq(-5,-1, by=1))
  cost_value_newPTCA = c(10^seq(-3,1, by=1))
  svm_df_newPTCA = data.frame()
    for(g in gamma_value_newPTCA){
      for(c in cost_value_newPTCA){
        # train model
        tmp_svm_in_iter_newPTCA = svm(as.factor(newPTCA) ~ . - newMI - newCABG - death - any.event,
                         kernel="radial",
                         gamma=g,
                         cost=c,
                         data = svm_validate_newPTCA)
        # validate model
        Predicted_y_in_iter_newPTCA <- predict(tmp_svm_in_iter_newPTCA, svm_test)
        # Compare Observed and Predicted
        table_svm_newPTCA <- prop.table(table(pred = Predicted_y_in_iter_newPTCA,
                    true = svm_test$newPTCA)/length(svm_test$newPTCA),1)
        in_df_newPTCA <- data.frame(gamma=g,
                      cost=c,
                      accuracy=table_svm_newPTCA[4] )
        svm_df_newPTCA <- rbind(svm_df_newPTCA,in_df_newPTCA)
      }
    }
  svm_df_newPTCA
}
svm_simulate_out_newPTCA = svm_simulate_newPTCA()
svm_simulate_out_newPTCA
svm_simulate_out_newPTCA_noNons = svm_simulate_out_newPTCA[complete.cases(svm_simulate_out_newPTCA),]

g_newPTCA = tail(filter(svm_simulate_out_newPTCA, accuracy == apply(svm_simulate_out_newPTCA_noNons, 2, max)[3])[1], 1)
c_newPTCA = tail(filter(svm_simulate_out_newPTCA, accuracy == apply(svm_simulate_out_newPTCA_noNons, 2, max)[3])[2], 1)

tmp_svm_proper_newPTCA = svm(as.factor(newPTCA) ~ . - newMI - newCABG - death - any.event, kernel="radial", gamma=g_newPTCA, cost=c_newPTCA, data=svm_develop_newPTCA)

Predicted_y_proper_newPTCA = predict(tmp_svm_proper_newPTCA, svm_test)


confusionMatrix(data = factor(Predicted_y_proper_newPTCA), reference = factor(svm_test$newMI))

##ROC Curve
##Not working properly
#svm_ypredscore_newPTCA = predict(tmp_svm_proper_newPTCA, svm_test, type = "decision")
#svm_perf_obj_newPTCA = prediction(as.numeric(svm_ypredscore_newPTCA), svm_test[,23])

#svm_perf_newPTCA = performance(svm_perf_obj_newPTCA, measure = "tpr", x.measure = "fpr")
#plot(svm_perf_newPTCA,
     #main="Cross-Sell - ROC Curves for newMI",
     #xlab="1 – Specificity: False Positive Rate",
     #ylab="Sensitivity: True Positive Rate",
     #col="blue")
#abline(0,1,col="grey")


##AUC 
svm_auc_newPTCA = roc.curve(svm_test$newPTCA, Predicted_y_proper_newPTCA)
svm_auc_newPTCA


##Using Gini Index and Lorenz curve to view the inequality among the population of our data
svm_ypredscore_newPTCA = predict(tmp_svm_proper_newPTCA, svm_test, type = "decision")
svm_gini_index_newPTCA = ineq(as.numeric(svm_ypredscore_newPTCA), type = "Gini")
svm_gini_index_newPTCA

dev.new()
plot(Lc(as.numeric(svm_ypredscore_newPTCA)), col = "darkred", lwd = 2)











##CLASSIFICATION USING SVM ON newCABG
##balancing the dependent variable
svm_train_rose_newCABG = ROSE(newCABG ~ . -death - newPTCA - newMI - any.event, data = svm_train)$data
table(svm_train_rose_newCABG$newCABG)

svm_validate_newCABG <- svm_train_rose_newCABG[sample(nrow(svm_train_rose_newCABG),size=20000, replace=TRUE),]
svm_develop_newCABG <- svm_train_rose_newCABG[sample(nrow(svm_train_rose_newCABG),size=20000, replace=TRUE),]

table(svm_validate_newCABG$newCABG)/length(svm_validate_newCABG$newCABG)

tmp_svm_newCABG = svm(as.factor(newCABG) ~ . - newMI - newPTCA - death - any.event, data = svm_validate_newCABG)
summary(tmp_svm_newCABG)


##prediction trained model
Predicted_y_newCABG <- predict(tmp_svm_newCABG, svm_test)
table_svm_newCABG <- table(pred = Predicted_y_newCABG, true = svm_test$newCABG)/length(svm_test$newCABG)
table_svm_newCABG

##We apply a function that will allow us get different kernel functions, gamma and cost values, and ultimately get the get combiunation of values for gamma and cost
##The computation cost of this seems high but it does the job well
svm_simulate_newCABG  = function(){
  gamma_value_newCABG  = c(10^seq(-5,-1, by=1))
  cost_value_newCABG = c(10^seq(-3,1, by=1))
  svm_df_newCABG = data.frame()
    for(g in gamma_value_newCABG){
      for(c in cost_value_newCABG){
        # train model
        tmp_svm_in_iter_newCABG = svm(as.factor(newCABG) ~ . - newMI - newPTCA - death - any.event,
                         kernel="radial",
                         gamma=g,
                         cost=c,
                         data = svm_validate_newCABG)
        # validate model
        Predicted_y_in_iter_newCABG <- predict(tmp_svm_in_iter_newCABG, svm_test)
        # Compare Observed and Predicted
        table_svm_newCABG <- prop.table(table(pred = Predicted_y_in_iter_newCABG,
                    true = svm_test$newCABG)/length(svm_test$newCABG),1)
        in_df_newCABG <- data.frame(gamma=g,
                      cost=c,
                      accuracy=table_svm_newCABG[4] )
        svm_df_newCABG <- rbind(svm_df_newCABG,in_df_newCABG)
      }
    }
  svm_df_newCABG
}
svm_simulate_out_newCABG = svm_simulate_newCABG()
svm_simulate_out_newCABG
svm_simulate_out_newCABG_noNons = svm_simulate_out_newCABG[complete.cases(svm_simulate_out_newCABG),]

g_newCABG = tail(filter(svm_simulate_out_newCABG, accuracy == apply(svm_simulate_out_newCABG_noNons, 2, max)[3])[1], 1)
c_newCABG = tail(filter(svm_simulate_out_newCABG, accuracy == apply(svm_simulate_out_newCABG_noNons, 2, max)[3])[2], 1)

tmp_svm_proper_newCABG = svm(as.factor(newCABG) ~ . - newMI - newPTCA - death - any.event, kernel="radial", gamma=g_newCABG, cost=c_newCABG, data=svm_develop_newCABG)

Predicted_y_proper_newCABG = predict(tmp_svm_proper_newCABG, svm_test)

confusionMatrix(data = factor(Predicted_y_proper_newCABG), reference = factor(svm_test$newMI))

##ROC Curve
##Not working properly
#svm_ypredscore_newCABG = predict(tmp_svm_proper_newCABG, svm_test, type="decision")
#svm_perf_obj_newCABG = prediction(as.numeric(svm_ypredscore_newCABG), svm_test[,24])

#svm_perf_newCABG = performance(svm_perf_obj_newCABG, measure = "tpr", x.measure = "fpr")
#plot(svm_perf_newCABG,
     #main="Cross-Sell - ROC Curves for newMI",
     #xlab="1 – Specificity: False Positive Rate",
     #ylab="Sensitivity: True Positive Rate",
     #col="blue")
#abline(0,1,col="grey")


##AUC 
svm_auc_newCABG = roc.curve(svm_test$newCABG, Predicted_y_proper_newCABG)
svm_auc_newCABG


##Using Gini Index and Lorenz curve to view the inequality among the population of our data
svm_ypredscore_newCABG = predict(tmp_svm_proper_newCABG, svm_test, type="decision")
svm_gini_index_newCABG = ineq(as.numeric(svm_ypredscore_newCABG), type = "Gini")
svm_gini_index_newCABG

dev.new()
plot(Lc(as.numeric(svm_ypredscore_newCABG)), col = "darkred", lwd = 2)











##CLASSIFICATION USING SVM ON death
##balancing the dependent variable
svm_train_rose_death = ROSE(death ~ . -newCABG - newPTCA - newMI - any.event, data = svm_train)$data
table(svm_train_rose_death$death)

svm_validate_death <- svm_train_rose_death[sample(nrow(svm_train_rose_death),size=20000, replace=TRUE),]
svm_develop_death <- svm_train_rose_death[sample(nrow(svm_train_rose_death),size=20000, replace=TRUE),]

table(svm_validate_death$death)/length(svm_validate_death$death)

tmp_svm_death = svm(as.factor(death) ~ . - newMI - newPTCA - newCABG - any.event, data = svm_validate_death)
summary(tmp_svm_death)


##prediction with trained model
Predicted_y_death <- predict(tmp_svm_death, svm_test)
table_svm_death <- table(pred = Predicted_y_death, true = svm_test$death)/length(svm_test$death)
table_svm_death

##We apply a function that will allow us get different kernel functions, gamma and cost values, and ultimately get the get combiunation of values for gamma and cost
##The computation cost of this seems high but it does the job well
svm_simulate_death  = function(){
  gamma_value_death  = c(10^seq(-5,-1, by=1))
  cost_value_death = c(10^seq(-3,1, by=1))
  svm_df_death = data.frame()
    for(g in gamma_value_death){
      for(c in cost_value_death){
        # train model
        tmp_svm_in_iter_death = svm(as.factor(death) ~ . - newMI - newPTCA - newCABG - any.event,
                         kernel="radial",
                         gamma=g,
                         cost=c,
                         data = svm_validate_death)
        # validate model
        Predicted_y_in_iter_death <- predict(tmp_svm_in_iter_death, svm_test)
        # Compare Observed and Predicted
        table_svm_death <- prop.table(table(pred = Predicted_y_in_iter_death,
                    true = svm_test$death)/length(svm_test$death),1)
        in_df_death <- data.frame(gamma=g,
                      cost=c,
                      accuracy=table_svm_death[4] )
        svm_df_death <- rbind(svm_df_death,in_df_death)
      }
    }
  svm_df_death
}
svm_simulate_out_death = svm_simulate_death()
svm_simulate_out_death
svm_simulate_out_death_noNons = svm_simulate_out_death[complete.cases(svm_simulate_out_death),]

g_death = tail(filter(svm_simulate_out_death, accuracy == apply(svm_simulate_out_death_noNons, 2, max)[3])[1], 1)
c_death = tail(filter(svm_simulate_out_death, accuracy == apply(svm_simulate_out_death_noNons, 2, max)[3])[2], 1)

tmp_svm_proper_death = svm(as.factor(death) ~ . - newMI - newPTCA - newCABG - any.event, kernel="radial", gamma=g_death, cost=c_death, data=svm_develop_death)

Predicted_y_proper_death = predict(tmp_svm_proper_death, svm_test)

confusionMatrix(data = factor(Predicted_y_proper_death), reference = factor(svm_test$death))

##ROC Curve
#svm_ypredscore_death = predict(tmp_svm_proper_death, svm_test, type="decision")
#svm_perf_obj_death = prediction(as.numeric(svm_ypredscore_death), svm_test[,24])

#svm_perf_death = performance(svm_perf_obj_death, measure = "tpr", x.measure = "fpr")
#dev.new()
#plot(svm_perf_death,
     #main="Cross-Sell - ROC Curves for newMI",
     #xlab="1 – Specificity: False Positive Rate",
     #ylab="Sensitivity: True Positive Rate",
     #col="blue")
#abline(0,1,col="grey")


##AUC 
svm_auc_death = roc.curve(svm_test$death, Predicted_y_proper_death)
svm_auc_death


##Using Gini Index and Lorenz curve to view the inequality among the population of our data
svm_ypredscore_death = predict(tmp_svm_proper_death, svm_test, type="decision")
svm_gini_index_death = ineq(as.numeric(svm_ypredscore_death), type = "Gini")
svm_gini_index_death

dev.new()
plot(Lc(as.numeric(svm_ypredscore_death)), col = "darkred", lwd = 2)











##CLASSIFICATION USING NEURAL NETWORKS
##Just like we did in the other ML algorithms for the dependent variable, we will go ahead and balance the dependent variables before we use them to model. 
nnet = read.csv("http://www.stat.ucla.edu/projects/datasets/cardiac.dat")
head(nnet)
nnet_mat = as.matrix(nnet)

##Due to the unbalance nature of our independent variable, we will have to balance the data using ROSE package

##Next, we dichotomize our data to train and test data set. 
nnet_smp_size = floor(0.75 * nrow(nnet))
nnet_train_ind = sample(seq_len(nrow(nnet)), size = nnet_smp_size)
nnet_train = nnet[nnet_train_ind,]
nnet_test = nnet[-nnet_train_ind,]

##Using the ROSE function to balance the unbalance dependent variable: newMI 
nnet_train_rose_newMI = ROSE(newMI ~ . -newCABG - newPTCA - death - any.event, data = nnet_train)$data
table(nnet_train_rose_newMI$newMI)

##Fit Single Hidden Layer Neural Network using least Squares
nnet_singHid_mod_newMI = nnet(as.factor(newMI) ~ . -newCABG - newPTCA - death - any.event, data = nnet_train_rose_newMI, size = 3, rang = 0.07, Hess = FALSE, decay = 15e-4, maxit = 250)

nnet_singHid_pred_newMI = predict(nnet_singHid_mod_newMI, nnet_test, type = ("class"))

confusionMatrix(data = factor(nnet_singHid_pred_newMI), reference = factor(nnet_test$newMI))

nnet_singHid_auc_newMI = roc.curve(nnet_test$newMI, nnet_singHid_pred_newMI)
nnet_singHid_auc_newMI
#Area under the curve (AUC): 0.500


##Using the ROSE function to balance the unbalance dependent variable: newPTCA 
nnet_train_rose_newPTCA = ROSE(newPTCA ~ . -newCABG - newMI - death - any.event, data = nnet_train)$data
table(nnet_train_rose_newPTCA$newPTCA)

##Fit Single Hidden Layer Neural Network using least Squares
nnet_singHid_mod_newPTCA = nnet(as.factor(newPTCA) ~ . -newCABG - newMI - death - any.event, data = nnet_train_rose_newPTCA, size = 3, rang = 0.07, Hess = FALSE, decay = 15e-4, maxit = 250)

nnet_singHid_pred_newPTCA = predict(nnet_singHid_mod_newPTCA, nnet_test, type = ("class"))

confusionMatrix(data = factor(nnet_singHid_pred_newPTCA), reference = factor(nnet_test$newPTCA))

nnet_singHid_auc_newPTCA = roc.curve(nnet_test$newPTCA, nnet_singHid_pred_newPTCA)
nnet_singHid_auc_newPTCA
#Area under the curve (AUC): 0.563



##Using the ROSE function to balance the unbalance dependent variable: newCABG 
nnet_train_rose_newCABG = ROSE(newCABG ~ . -newPTCA - newMI - death - any.event, data = nnet_train)$data
table(nnet_train_rose_newCABG$newCABG)

##Fit Single Hidden Layer Neural Network using least Squares
nnet_singHid_mod_newCABG = nnet(as.factor(newCABG) ~ . -newPTCA - newMI - death - any.event, data = nnet_train_rose_newCABG, size = 3, rang = 0.07, Hess = FALSE, decay = 15e-4, maxit = 250)

nnet_singHid_pred_newCABG = predict(nnet_singHid_mod_newCABG, nnet_test, type = ("class"))

confusionMatrix(data = factor(nnet_singHid_pred_newCABG), reference = factor(nnet_test$newCABG))

nnet_singHid_auc_newCABG = roc.curve(nnet_test$newCABG, nnet_singHid_pred_newCABG)
nnet_singHid_auc_newCABG
#Area under the curve (AUC): 0.500


##Using the ROSE function to balance the unbalance dependent variable: death 
nnet_train_rose_death = ROSE(death ~ . -newPTCA - newMI - newCABG - any.event, data = nnet_train)$data
table(nnet_train_rose_death$death)

##Fit Single Hidden Layer Neural Network using least Squares
nnet_singHid_mod_death = nnet(as.factor(death) ~ . -newCABG - newMI - newPTCA - any.event, data = nnet_train_rose_death, size = 3, rang = 0.07, Hess = FALSE, decay = 15e-4, maxit = 250)

nnet_singHid_pred_death = predict(nnet_singHid_mod_death, nnet_test, type = ("class"))

confusionMatrix(data = factor(nnet_singHid_pred_death), reference = factor(nnet_test$death))

nnet_singHid_auc_death = roc.curve(nnet_test$death, nnet_singHid_pred_death)
nnet_singHid_auc_death
#Area under the curve (AUC): 0.850










##Fit using multinomial Log Linear neural network for newMI
nnet_mulLogLin_mod_newMI = multinom(as.factor(newMI) ~ . -newCABG - newPTCA - death - any.event, data = nnet_train_rose_newMI)

nnet_mulLogLin_pred_newMI = predict(nnet_mulLogLin_mod_newMI, nnet_test)

confusionMatrix(data = factor(nnet_mulLogLin_pred_newMI), reference = factor(nnet_test$newMI))

nnet_mulLogLin_auc_newMI = roc.curve(nnet_test$newMI, nnet_mulLogLin_pred_newMI)
nnet_mulLogLin_auc_newMI
##Area under the curve (AUC): 0.955

##Fit using multinomial Log Linear neural network for newPTCA
nnet_train_rose_newPTCA = ROSE(newPTCA ~ . -newCABG - newMI - death - any.event, data = nnet_train)$data
table(nnet_train_rose_newPTCA$newPTCA)
nnet_mulLogLin_mod_newPTCA = multinom(as.factor(newPTCA) ~ . -newCABG - newMI - death - any.event, data = nnet_train_rose_newPTCA)

nnet_mulLogLin_pred_newPTCA = predict(nnet_mulLogLin_mod_newPTCA, nnet_test)

confusionMatrix(data = factor(nnet_mulLogLin_pred_newPTCA), reference = factor(nnet_test$newPTCA))

nnet_mulLogLin_auc_newPTCA = roc.curve(nnet_test$newPTCA, nnet_mulLogLin_pred_newPTCA)
nnet_mulLogLin_auc_newPTCA
#Area under the curve (AUC): 0.822

##Fit using multinomial Log Linear neural network for newCABG
nnet_train_rose_newCABG = ROSE(newCABG ~ . -newPTCA - newMI - death - any.event, data = nnet_train)$data
table(nnet_train_rose_newCABG$newCABG)
nnet_mulLogLin_mod_newCABG = multinom(as.factor(newCABG) ~ . -newPTCA - newMI - death - any.event, data = nnet_train_rose_newCABG)

nnet_mulLogLin_pred_newCABG = predict(nnet_mulLogLin_mod_newCABG, nnet_test)

confusionMatrix(data = factor(nnet_mulLogLin_pred_newCABG), reference = factor(nnet_test$newCABG))

nnet_mulLogLin_auc_newCABG = roc.curve(nnet_test$newCABG, nnet_mulLogLin_pred_newCABG)
nnet_mulLogLin_auc_newCABG
#Area under the curve (AUC): 0.822


##Fit using multinomial Log Linear neural network for death
nnet_train_rose_death = ROSE(death ~ . -newPTCA - newMI - newCABG - any.event, data = nnet_train)$data
table(nnet_train_rose_death$death)
nnet_mulLogLin_mod_death = multinom(as.factor(death) ~ . -newPTCA - newMI - newCABG - any.event, data = nnet_train_rose_death)

nnet_mulLogLin_pred_death = predict(nnet_mulLogLin_mod_death, nnet_test)

confusionMatrix(data = factor(nnet_mulLogLin_pred_death), reference = factor(nnet_test$death))

nnet_mulLogLin_auc_death = roc.curve(nnet_test$death, nnet_mulLogLin_pred_death)
nnet_mulLogLin_auc_death
#Area under the curve (AUC): 0.981







##Fit neural network using Back Propagation
str(nnet_train)

columns = c("bhr", "basebp", "basedp", "pkhr", "sbp", "dp","dose","maxhr","X.mphr.b.","mbp", "dpmaxdo", "dobdose","age","gender","baseEF","dobEF", "chestpain", "posECG", "equivecg",  "restwma", "posSE","hxofHT", "hxofdm", "hxofcig", "hxofMI", "hxofPTCA", "hxofCABG", "phat", "event...", "mics", "deltaEF", "newpkmphr", "gdpkmphr", "gdmaxmphr", "gddpeakdp", "gdmaxdp", "hardness")

nnet_test_new = subset(nnet_test, select = columns)

##Make sure all variables are either interger or numerical
##neural network using Back Propagation: newMI
nnet_bpa_mod_newMI = neuralnet(newMI ~ bhr + basebp + basedp + pkhr + sbp + dp + dose + maxhr + X.mphr.b. + mbp + dpmaxdo + dobdose + age + gender + baseEF + dobEF + chestpain + posECG +  equivecg + restwma + posSE + hxofHT + hxofdm + hxofcig + hxofMI + hxofPTCA + hxofCABG + phat + event... + mics + deltaEF + newpkmphr + gdpkmphr + gdmaxmphr + gddpeakdp + gdmaxdp + hardness, data = nnet_train, hidden = 5, threshold = 0.01, err.fct = "sse", linear.output = FALSE, likelihood = "TRUE", stepmax = 1e+05, rep = 1, startweights = NULL, learningrate.limit = list(0.1,1.5), learningrate.factor = list(minus = 0.5, plus = 1.5), learningrate = 0.5, lifesign.step = 1000, algorithm = "backprop", act.fct = "logistic", exclude = NULL, constant.weights = NULL)

summary(nnet_bpa_mod_newMI)
plot(nnet_bpa_mod_newMI, rep = "best", main = "")
title(main = "neural network using Back Propagation: newMI")

nnet_bpa_pred_newMI = compute(nnet_bpa_mod_newMI, nnet_test_new, rep = 1)

table(nnet_test$newMI, nnet_bpa_pred_newMI$net.result)

cbind(nnet_test$newMI, nnet_bpa_pred_newMI$net.result)


##AUC
nnet_bpa_auc_newMI = roc.curve(nnet_test$newMI, nnet_bpa_pred_newMI$net.result)
nnet_bpa_auc_newMI
#Area under the curve (AUC): 0.504


##neural network using Back Propagation:newPTCA
nnet_bpa_mod_newPTCA = neuralnet(newPTCA ~ bhr + basebp + basedp + pkhr + sbp + dp + dose + maxhr + X.mphr.b. + mbp + dpmaxdo + dobdose + age + gender + baseEF + dobEF + chestpain + posECG +  equivecg + restwma + posSE + hxofHT + hxofdm + hxofcig + hxofMI + hxofPTCA + hxofCABG + phat + event... + mics + deltaEF + newpkmphr + gdpkmphr + gdmaxmphr + gddpeakdp + gdmaxdp + hardness, data = nnet_train, hidden = 5, threshold = 0.01, err.fct = "sse", linear.output = FALSE, likelihood = "TRUE", stepmax = 1e+05, rep = 1, startweights = NULL, learningrate.limit = list(0.1,1.5), learningrate.factor = list(minus = 0.5, plus = 1.5), learningrate = 0.5, lifesign.step = 1000, algorithm = "backprop", act.fct = "logistic", exclude = NULL, constant.weights = NULL)

summary(nnet_bpa_mod_newPTCA)
plot(nnet_bpa_mod_newPTCA, rep = "best", main = "")
title(main = "neural network using Back Propagation:newPTCA")

nnet_bpa_pred_newPTCA = compute(nnet_bpa_mod_newPTCA, nnet_test_new, rep = 1)

table(nnet_test$newPTCA, nnet_bpa_pred_newPTCA$net.result)

cbind(nnet_test$newPTCA, nnet_bpa_pred_newPTCA$net.result)


##AUC
nnet_bpa_auc_newPTCA = roc.curve(nnet_test$newPTCA, nnet_bpa_pred_newPTCA$net.result)
nnet_bpa_auc_newPTCA
#Area under the curve (AUC): 0.504



##neural network using Back Propagation: newCABG
nnet_bpa_mod_newCABG = neuralnet(newCABG ~ bhr + basebp + basedp + pkhr + sbp + dp + dose + maxhr + X.mphr.b. + mbp + dpmaxdo + dobdose + age + gender + baseEF + dobEF + chestpain + posECG +  equivecg + restwma + posSE + hxofHT + hxofdm + hxofcig + hxofMI + hxofPTCA + hxofCABG + phat + event... + mics + deltaEF + newpkmphr + gdpkmphr + gdmaxmphr + gddpeakdp + gdmaxdp + hardness, data = nnet_train, hidden = 5, threshold = 0.01, err.fct = "sse", linear.output = FALSE, likelihood = "TRUE", stepmax = 1e+05, rep = 1, startweights = NULL, learningrate.limit = list(0.1,1.5), learningrate.factor = list(minus = 0.5, plus = 1.5), learningrate = 0.5, lifesign.step = 1000, algorithm = "backprop", act.fct = "logistic", exclude = NULL, constant.weights = NULL)

summary(nnet_bpa_mod_newCABG)
plot(nnet_bpa_mod_newCABG, rep = "best", main = "")
title(main = "neural network using Back Propagation: newCABG")

nnet_bpa_pred_newCABG = compute(nnet_bpa_mod_newCABG, nnet_test_new, rep = 1)

table(nnet_test$newCABG, nnet_bpa_pred_newCABG$net.result)

cbind(nnet_test$newCABG, nnet_bpa_pred_newCABG$net.result)


##AUC
nnet_bpa_auc_newCABG = roc.curve(nnet_test$newCABG, nnet_bpa_pred_newCABG$net.result)
nnet_bpa_auc_newCABG
#Area under the curve (AUC): 0.500


##neural network using Back Propagation: death
nnet_bpa_mod_death = neuralnet(death ~ bhr + basebp + basedp + pkhr + sbp + dp + dose + maxhr + X.mphr.b. + mbp + dpmaxdo + dobdose + age + gender + baseEF + dobEF + chestpain + posECG +  equivecg + restwma + posSE + hxofHT + hxofdm + hxofcig + hxofMI + hxofPTCA + hxofCABG + phat + event... + mics + deltaEF + newpkmphr + gdpkmphr + gdmaxmphr + gddpeakdp + gdmaxdp + hardness, data = nnet_train, hidden = 5, threshold = 0.01, err.fct = "sse", linear.output = FALSE, likelihood = "TRUE", stepmax = 1e+05, rep = 1, startweights = NULL, learningrate.limit = list(0.1,1.5), learningrate.factor = list(minus = 0.5, plus = 1.5), learningrate = 0.5, lifesign.step = 1000, algorithm = "backprop", act.fct = "logistic", exclude = NULL, constant.weights = NULL)

summary(nnet_bpa_mod_death)
plot(nnet_bpa_mod_death, rep = "best", main = "")
title(main = "neural network using Back Propagation: death")

nnet_bpa_pred_death = compute(nnet_bpa_mod_death, nnet_test_new, rep = 1)

table(nnet_test$death, nnet_bpa_pred_death$net.result)

cbind(nnet_test$death, nnet_bpa_pred_death$net.result)


##AUC
nnet_bpa_auc_death = roc.curve(nnet_test$death, nnet_bpa_pred_death$net.result)
nnet_bpa_auc_death












##CLASSIFICATION USING XGBOOST
xgb = read.csv("http://www.stat.ucla.edu/projects/datasets/cardiac.dat")
head(xgb)
xgb_mat = as.matrix(xgb)


##Next, we dichotomize our data to train and test data set. 
xgb_smp_size = floor(0.75 * nrow(xgb))
xgb_train_ind = sample(seq_len(nrow(xgb)), size = xgb_smp_size)
xgb_train = xgb[xgb_train_ind,]
xgb_test = xgb[-xgb_train_ind,]

##XGBOOST CLASSIFICATION FOR newMI
xgb_train_rose_newMI = ROSE(newMI ~ . -newCABG - newPTCA - death - any.event, data = xgb_train)$data
table(xgb_train_rose_newMI$newMI)

xgb_y_train_newMI = as.matrix(xgb_train_rose_newMI[,22])
xgb_x_train_newMI = as.matrix(xgb_train_rose_newMI[,-c(22,23,24,25,32)])
xgb_y_test_newMI = as.matrix(xgb_test[,22])
xgb_x_test_newMI = as.matrix(xgb_test[,-c(22,23,24,25,32)])

#xgb_y_train_newMI = as(xgb_y_train_newMI, "sparseMatrix")
#xgb_x_train_newMI = as(xgb_x_train_newMI, "sparseMatrix")
#xgb_y_test_newMI = as(xgb_y_test_newMI, "sparseMatrix")
#xgb_x_test_newMI = as(xgb_x_test_newMI, "sparseMatrix")


xgb_train_data_newMI = xgb.DMatrix(data = xgb_x_train_newMI, label = xgb_y_train_newMI)


##The tuning process will go through 30 iterations in search for the best parameter levels to optimize our model
All_rmse_newMI = c()
Param_group_newMI = c()
for (idx in 1:5){
  param_newMI = list(objective = "binary:logistic",
         eval_metric = "rmse",
         booster = "gbtree",
         max_depth = sample(2:10, 1),
         eta = runif(1, 0.01, 0.3),
         gamma = runif(1, 0.0, 0.2),
         subsample = runif(1, 0.6, 0.9),
         colsample_bytree = runif(1, 0.5, 0.8)
         )
  
  cv_nround_newMI = 800
  cv_nfold_newMI = 6
  cv_nthread_newMI = 6
  
  cv_xgb_newMI = xgb.cv( params = param_newMI,
               data = xgb_x_train_newMI,
               label = xgb_y_train_newMI, 
               nthread = cv_nthread_newMI,
               nfold = cv_nfold_newMI,
               prediction = TRUE,
               nrounds = cv_nround_newMI,
               verbose = 2)
          
  min_rmse_newMI = min(cv_xgb_newMI$dt[,test.rmse.mean])
  All_rmse_newMI = append(All_rmse_newMI, min_rmse_newMI)
  Param_group_newMI = append(Param_group_newMI, param_newMI)
  #best parameter
  param_min_newMI = Param_group_newMI[(which.min(All_rmse_newMI)*8+1): (which.min(All_rmse_newMI)*8+8)]
}

param_newMI

optimised_params_newMI = list(objective = param_newMI$objective,
               eval_metric = param_newMI$eval_metric,
             booster = param_newMI$booster,
               max_depth = param_newMI$max_depth,
               eta = param_newMI$eta,
               gamma = param_newMI$gamma,
               subsample = param_newMI$subsample,
               colsample_bytree = param_newMI$colsample_bytree
               )  
                             
tail(cv_xgb_newMI$dt)              
min_mean_idx_newMI = which.min(cv_xgb_newMI$dt[, test.rmse.mean]) 
min_mean_idx_newMI
cv_xgb_newMI$dt[min_mean_idx_newMI,]  
num_class_newMI = length(levels(as.factor(xgb_y_train_newMI)))

##Confusion Matrix
pred_cv_newMI = matrix(cv_xgb_newMI$pred, ncol = length(cv_xgb_newMI$pred), nrow = num_class_newMI)
pred_cv_newMI = t(pred_cv_newMI)
pred_cv_max_newMI = max.col(pred_cv_newMI, "last")
conf_matrix_newMI = confusionMatrix(factor(xgb_y_train_newMI+1), factor(pred_cv_max_newMI))
                       
            
xgb_model_newMI = xgb.train(params = optimised_params_newMI,
               data = xgb_train_data_newMI,
               nrounds = 800,
               watchlist = list(train = xgb_train_data_newMI),
               verbose = 2,
               print_every_n = 50,
               nthread = 6
               )
               
importance_matrix_newMI = xgb.importance(model = xgb_model_newMI)
print(importance_matrix_newMI)
xgb.plot.importance(importance_matrix = importance_matrix_newMI)

#xgb_x_test_data_newMI  = xgb.DMatrix(data = xgb_x_test_newMI)
#xgb_y_test_data_newMI  = xgb.DMatrix(data = xgb_y_test_newMI)

##Prediction
xgb_predict_newMI = round(predict(xgb_model_newMI, xgb_x_test_newMI),0)

conf_matrix_newMI = confusionMatrix(factor(xgb_y_test_newMI), factor(xgb_predict_newMI))

##AUC 
dev.new()
xgb_auc_newMI = roc.curve(xgb_y_test_newMI, xgb_predict_newMI)
xgb_auc_newMI

print(xgb_y_test_newMI, xgb_predict_newMI)









##XGBOOST CLASSIFICATION FOR newPTCA
xgb_train_rose_newPTCA = ROSE(newPTCA ~ . -newCABG - newMI - death - any.event, data = xgb_train)$data
table(xgb_train_rose_newPTCA$newPTCA)

xgb_y_train_newPTCA = as.matrix(xgb_train_rose_newPTCA[,23])
xgb_x_train_newPTCA = as.matrix(xgb_train_rose_newPTCA[,-c(22,23,24,25,32)])
xgb_y_test_newPTCA = as.matrix(xgb_test[,23])
xgb_x_test_newPTCA = as.matrix(xgb_test[,-c(22,23,24,25,32)])

#xgb_y_train_newMI = as(xgb_y_train_newPTCA, "sparseMatrix")
#xgb_x_train_newMI = as(xgb_x_train_newPTCA, "sparseMatrix")
#xgb_y_test_newMI = as(xgb_y_test_newPTCA, "sparseMatrix")
#xgb_x_test_newMI = as(xgb_x_test_newPTCA, "sparseMatrix")


xgb_train_data_newPTCA = xgb.DMatrix(data = xgb_x_train_newPTCA, label = xgb_y_train_newPTCA)


##The tuning process will go through 30 iterations in search for the best parameter levels to optimize our model
All_rmse_newPTCA = c()
Param_group_newPTCA = c()
for (idx in 1:5){
  param_newPTCA = list(objective = "binary:logistic",
         eval_metric = "rmse",
         booster = "gbtree",
         max_depth = sample(2:10, 1),
         eta = runif(1, 0.01, 0.3),
         gamma = runif(1, 0.0, 0.2),
         subsample = runif(1, 0.6, 0.9),
         colsample_bytree = runif(1, 0.5, 0.8)
         )
  
  cv_nround_newPTCA = 800
  cv_nfold_newPTCA = 6
  cv_nthread_newPTCA = 6
  
  cv_xgb_newPTCA = xgb.cv( params = param_newPTCA,
               data = xgb_x_train_newPTCA,
               label = xgb_y_train_newPTCA, 
               nthread = cv_nthread_newPTCA,
               nfold = cv_nfold_newPTCA,
               prediction = TRUE,
               nrounds = cv_nround_newPTCA,
               verbose = 2)
          
  min_rmse_newPTCA = min(cv_xgb_newPTCA$dt[,test.rmse.mean])
  All_rmse_newPTCA = append(All_rmse_newPTCA, min_rmse_newPTCA)
  Param_group_newPTCA = append(Param_group_newPTCA, param_newPTCA)
  #best parameter
  param_min_newPTCA = Param_group_newPTCA[(which.min(All_rmse_newPTCA)*8+1): (which.min(All_rmse_newPTCA)*8+8)]
}

param_newPTCA

optimised_params_newPTCA = list(objective = param_newPTCA$objective,
               eval_metric = param_newPTCA$eval_metric,
             booster = param_newPTCA$booster,
               max_depth = param_newPTCA$max_depth,
               eta = param_newPTCA$eta,
               gamma = param_newPTCA$gamma,
               subsample = param_newPTCA$subsample,
               colsample_bytree = param_newPTCA$colsample_bytree
               )  
                             
tail(cv_xgb_newPTCA$dt)              
min_mean_idx_newPTCA = which.min(cv_xgb_newPTCA$dt[, test.rmse.mean]) 
min_mean_idx_newPTCA
cv_xgb_newPTCA$dt[min_mean_idx_newPTCA,]  
num_class_newPTCA = length(levels(as.factor(xgb_y_train_newPTCA)))

##Confusion Matrix
pred_cv_newPTCA = matrix(cv_xgb_newPTCA$pred, ncol = length(cv_xgb_newPTCA$pred), nrow = num_class_newPTCA)
pred_cv_newPTCA = t(pred_cv_newPTCA)
pred_cv_max_newPTCA = max.col(pred_cv_newPTCA, "last")
conf_matrix_newPTCA = confusionMatrix(factor(xgb_y_train_newPTCA+1), factor(pred_cv_max_newPTCA))
                       
            
xgb_model_newPTCA = xgb.train(params = optimised_params_newPTCA,
               data = xgb_train_data_newPTCA,
               nrounds = 800,
               watchlist = list(train = xgb_train_data_newPTCA),
               verbose = 2,
               print_every_n = 50,
               nthread = 6
               )
               
importance_matrix_newPTCA = xgb.importance(model = xgb_model_newPTCA)
print(importance_matrix_newPTCA)
xgb.plot.importance(importance_matrix = importance_matrix_newPTCA)

#xgb_x_test_data_newMI  = xgb.DMatrix(data = xgb_x_test_newMI)
#xgb_y_test_data_newMI  = xgb.DMatrix(data = xgb_y_test_newMI)

##Prediction
xgb_predict_newPTCA = round(predict(xgb_model_newPTCA, xgb_x_test_newPTCA),0)

conf_matrix_newPTCA = confusionMatrix(factor(xgb_y_test_newPTCA), factor(xgb_predict_newPTCA))

##AUC 
dev.new()
xgb_auc_newPTCA = roc.curve(xgb_y_test_newPTCA, xgb_predict_newPTCA)
xgb_auc_newPTCA
##Area under the curve (AUC): 0.946






##XGBOOST CLASSIFICATION FOR newCABG
xgb_train_rose_newCABG = ROSE(newCABG ~ . -newPTCA - newMI - death - any.event, data = xgb_train)$data
table(xgb_train_rose_newCABG$newCABG)

xgb_y_train_newCABG = as.matrix(xgb_train_rose_newCABG[,24])
xgb_x_train_newCABG = as.matrix(xgb_train_rose_newCABG[,-c(22,23,24,25,32)])
xgb_y_test_newCABG = as.matrix(xgb_test[,24])
xgb_x_test_newCABG = as.matrix(xgb_test[,-c(22,23,24,25,32)])

#xgb_y_train_newCABG = as(xgb_y_train_newCABG, "sparseMatrix")
#xgb_x_train_newCABG = as(xgb_x_train_newCABG, "sparseMatrix")
#xgb_y_test_newCABG = as(xgb_y_test_newCABG, "sparseMatrix")
#xgb_x_test_newCABG = as(xgb_x_test_newCABG, "sparseMatrix")


xgb_train_data_newCABG = xgb.DMatrix(data = xgb_x_train_newCABG, label = xgb_y_train_newCABG)


##The tuning process will go through 30 iterations in search for the best parameter levels to optimize our model
All_rmse_newCABG = c()
Param_group_newCABG = c()
for (idx in 1:5){
  param_newCABG = list(objective = "binary:logistic",
         eval_metric = "rmse",
         booster = "gbtree",
         max_depth = sample(2:10, 1),
         eta = runif(1, 0.01, 0.3),
         gamma = runif(1, 0.0, 0.2),
         subsample = runif(1, 0.6, 0.9),
         colsample_bytree = runif(1, 0.5, 0.8)
         )
  
  cv_nround_newCABG = 800
  cv_nfold_newCABG = 6
  cv_nthread_newCABG = 6
  
  cv_xgb_newCABG = xgb.cv( params = param_newCABG,
               data = xgb_x_train_newCABG,
               label = xgb_y_train_newCABG, 
               nthread = cv_nthread_newCABG,
               nfold = cv_nfold_newCABG,
               prediction = TRUE,
               nrounds = cv_nround_newCABG,
               verbose = 2)
          
  min_rmse_newCABG = min(cv_xgb_newCABG$dt[,test.rmse.mean])
  All_rmse_newCABG = append(All_rmse_newCABG, min_rmse_newCABG)
  Param_group_newCABG = append(Param_group_newCABG, param_newCABG)
  #best parameter
  param_min_newCABG = Param_group_newCABG[(which.min(All_rmse_newCABG)*8+1): (which.min(All_rmse_newCABG)*8+8)]
}

param_newCABG

optimised_params_newCABG = list(objective = param_newCABG$objective,
               eval_metric = param_newCABG$eval_metric,
             booster = param_newCABG$booster,
               max_depth = param_newCABG$max_depth,
               eta = param_newCABG$eta,
               gamma = param_newCABG$gamma,
               subsample = param_newCABG$subsample,
               colsample_bytree = param_newCABG$colsample_bytree
               )  
                             
tail(cv_xgb_newCABG$dt)              
min_mean_idx_newCABG = which.min(cv_xgb_newCABG$dt[, test.rmse.mean]) 
min_mean_idx_newCABG
cv_xgb_newCABG$dt[min_mean_idx_newCABG,]  
num_class_newCABG = length(levels(as.factor(xgb_y_train_newCABG)))

##Confusion Matrix
pred_cv_newCABG = matrix(cv_xgb_newCABG$pred, ncol = length(cv_xgb_newCABG$pred), nrow = num_class_newCABG)
pred_cv_newCABG = t(pred_cv_newCABG)
pred_cv_max_newCABG = max.col(pred_cv_newCABG, "last")
conf_matrix_newCABG = confusionMatrix(factor(xgb_y_train_newCABG+1), factor(pred_cv_max_newCABG))
                       
            
xgb_model_newCABG = xgb.train(params = optimised_params_newCABG,
               data = xgb_train_data_newCABG,
               nrounds = 800,
               watchlist = list(train = xgb_train_data_newCABG),
               verbose = 2,
               print_every_n = 50,
               nthread = 6
               )
               
importance_matrix_newCABG = xgb.importance(model = xgb_model_newCABG)
print(importance_matrix_newCABG)
xgb.plot.importance(importance_matrix = importance_matrix_newCABG)

#xgb_x_test_data_newMI  = xgb.DMatrix(data = xgb_x_test_newMI)
#xgb_y_test_data_newMI  = xgb.DMatrix(data = xgb_y_test_newMI)

##Prediction
xgb_predict_newCABG = round(predict(xgb_model_newCABG, xgb_x_test_newCABG),0)

conf_matrix_newCABG = confusionMatrix(factor(xgb_y_test_newCABG), factor(xgb_predict_newCABG))

##AUC 
dev.new()
xgb_auc_newCABG = roc.curve(xgb_y_test_newCABG, xgb_predict_newCABG)
xgb_auc_newCABG
##Area under the curve (AUC): 0.935









##XGBOOST CLASSIFICATION FOR death
xgb_train_rose_death = ROSE(death ~ . -newPTCA - newMI - newCABG - any.event, data = xgb_train)$data
table(xgb_train_rose_death$death)

xgb_y_train_death = as.matrix(xgb_train_rose_death[,25])
xgb_x_train_death = as.matrix(xgb_train_rose_death[,-c(22,23,24,25,32)])
xgb_y_test_death = as.matrix(xgb_test[,25])
xgb_x_test_death = as.matrix(xgb_test[,-c(22,23,24,25,32)])

#xgb_y_train_death = as(xgb_y_train_death, "sparseMatrix")
#xgb_x_train_death = as(xgb_x_train_death, "sparseMatrix")
#xgb_y_test_death = as(xgb_y_test_death, "sparseMatrix")
#xgb_x_test_death = as(xgb_x_test_death, "sparseMatrix")


xgb_train_data_death = xgb.DMatrix(data = xgb_x_train_death, label = xgb_y_train_death)


##The tuning process will go through 30 iterations in search for the best parameter levels to optimize our model
All_rmse_death = c()
Param_group_death = c()
for (idx in 1:5){
  param_death = list(objective = "binary:logistic",
         eval_metric = "rmse",
         booster = "gbtree",
         max_depth = sample(2:10, 1),
         eta = runif(1, 0.01, 0.3),
         gamma = runif(1, 0.0, 0.2),
         subsample = runif(1, 0.6, 0.9),
         colsample_bytree = runif(1, 0.5, 0.8)
         )
  
  cv_nround_death = 800
  cv_nfold_death = 6
  cv_nthread_death = 6
  
  cv_xgb_death = xgb.cv( params = param_death,
               data = xgb_x_train_death,
               label = xgb_y_train_death, 
               nthread = cv_nthread_death,
               nfold = cv_nfold_death,
               prediction = TRUE,
               nrounds = cv_nround_death,
               verbose = 2)
          
  min_rmse_death = min(cv_xgb_death$dt[,test.rmse.mean])
  All_rmse_death = append(All_rmse_death, min_rmse_death)
  Param_group_death = append(Param_group_death, param_death)
  #best parameter
  param_min_death = Param_group_death[(which.min(All_rmse_death)*8+1): (which.min(All_rmse_death)*8+8)]
}

param_death

optimised_params_death = list(objective = param_death$objective,
               eval_metric = param_death$eval_metric,
             booster = param_death$booster,
               max_depth = param_death$max_depth,
               eta = param_death$eta,
               gamma = param_death$gamma,
               subsample = param_death$subsample,
               colsample_bytree = param_death$colsample_bytree
               )  
                             
tail(cv_xgb_death$dt)              
min_mean_idx_death = which.min(cv_xgb_death$dt[, test.rmse.mean]) 
min_mean_idx_death
cv_xgb_death$dt[min_mean_idx_death,]  
num_class_death = length(levels(as.factor(xgb_y_train_death)))

##Confusion Matrix
pred_cv_death = matrix(cv_xgb_death$pred, ncol = length(cv_xgb_death$pred), nrow = num_class_death)
pred_cv_death = t(pred_cv_death)
pred_cv_max_death = max.col(pred_cv_death, "last")
conf_matrix_death = confusionMatrix(factor(xgb_y_train_death+1), factor(pred_cv_max_death))
                       
            
xgb_model_death = xgb.train(params = optimised_params_death,
               data = xgb_train_data_death,
               nrounds = 800,
               watchlist = list(train = xgb_train_data_death),
               verbose = 2,
               print_every_n = 50,
               nthread = 6
               )
               
importance_matrix_death = xgb.importance(model = xgb_model_death)
print(importance_matrix_death)
xgb.plot.importance(importance_matrix = importance_matrix_death)

#xgb_x_test_data_newMI  = xgb.DMatrix(data = xgb_x_test_newMI)
#xgb_y_test_data_newMI  = xgb.DMatrix(data = xgb_y_test_newMI)

##Prediction
xgb_predict_death = round(predict(xgb_model_death, xgb_x_test_death),0)

conf_matrix_death = confusionMatrix(factor(xgb_y_test_death), factor(xgb_predict_death))

##AUC 
dev.new()
xgb_auc_death = roc.curve(xgb_y_test_death, xgb_predict_death)
xgb_auc_death
##Area under the curve (AUC): 0.978









##CLASSIFICATION USING NAIVE BAYES
bay = read.csv("http://www.stat.ucla.edu/projects/datasets/cardiac.dat")
head(bay)
bay_mat = as.matrix(bay)


##Next, we dichotomize our data to train and test data set. 
bay_smp_size = floor(0.75 * nrow(bay))
bay_train_ind = sample(seq_len(nrow(bay)), size = bay_smp_size)
bay_train = bay[bay_train_ind,]
bay_test = bay[-bay_train_ind,]

##NAIVE BAYES CLASSIFICATION FOR newMI
bay_train_rose_newMI = ROSE(newMI ~ . -newCABG - newPTCA - death - any.event, data = bay_train)$data
table(bay_train_rose_newMI$newMI)

bay_model_newMI = naiveBayes(as.factor(newMI) ~ . -newCABG - newPTCA - death - any.event, data = bay_train_rose_newMI)

##Prediction
bay_pred_newMI = predict(bay_model_newMI, bay_test[,-c(22,23,24,25,32)])

bay_conf_matrix_newMI = confusionMatrix(factor(bay_test$newMI), factor(bay_pred_newMI))

bay_auc_newMI = roc.curve(bay_test$newMI, bay_pred_newMI)
bay_auc_newMI
#Area under the curve (AUC): 0.902







##NAIVE BAYES CLASSIFICATION FOR newPTCA
bay_train_rose_newPTCA = ROSE(newPTCA ~ . -newCABG - newMI - death - any.event, data = bay_train)$data
table(bay_train_rose_newPTCA$newPTCA)

bay_model_newPTCA = naiveBayes(as.factor(newPTCA) ~ . -newCABG - newMI - death - any.event, data = bay_train_rose_newPTCA)

##Prediction
bay_pred_newPTCA = predict(bay_model_newPTCA, bay_test[,-c(22,23,24,25,32)])

bay_conf_matrix_newPTCA = confusionMatrix(factor(bay_test$newPTCA), factor(bay_pred_newPTCA))

bay_auc_newPTCA = roc.curve(bay_test$newPTCA, bay_pred_newPTCA)
bay_auc_newPTCA
#Area under the curve (AUC): 0.960






##NAIVE BAYES CLASSIFICATION FOR newCABG
bay_train_rose_newCABG = ROSE(newCABG ~ . -newPTCA - newMI - death - any.event, data = bay_train)$data
table(bay_train_rose_newCABG$newCABG)

bay_model_newCABG = naiveBayes(as.factor(newCABG) ~ . -newPTCA - newMI - death - any.event, data = bay_train_rose_newCABG)

##Prediction
bay_pred_newCABG = predict(bay_model_newCABG, bay_test[,-c(22,23,24,25,32)])

bay_conf_matrix_newCABG = confusionMatrix(factor(bay_test$newCABG), factor(bay_pred_newCABG))

bay_auc_newCABG = roc.curve(bay_test$newCABG, bay_pred_newCABG)
bay_auc_newCABG
#Area under the curve (AUC): 0.956






##NAIVE BAYES CLASSIFICATION FOR death
bay_train_rose_death = ROSE(death ~ . -newPTCA - newMI - newCABG - any.event, data = bay_train)$data
table(bay_train_rose_death$death)

bay_model_death = naiveBayes(as.factor(death) ~ . -newPTCA - newMI - newCABG - any.event, data = bay_train_rose_death)

##Prediction
bay_pred_death = predict(bay_model_death, bay_test[,-c(22,23,24,25,32)])

bay_conf_matrix_death = confusionMatrix(factor(bay_test$death), factor(bay_pred_death))

bay_auc_death = roc.curve(bay_test$death, bay_pred_death)
bay_auc_death
#Area under the curve (AUC): 0.963




newMI_df = data.frame(ML_algo=character(),
                  AUC=integer(),
                  stringsAsFactors=FALSE)
newMI_df[1,1] = 'auc_curve_over'
newMI_df[2,1] = 'auc_curve_rose' 
newMI_df[3,1] = 'svm_auc_newMI'
newMI_df[4,1] = 'nnet_singHid_auc_newMI'
newMI_df[5,1] = 'nnet_mulLogLin_auc_newMI'
newMI_df[6,1] = 'nnet_bpa_auc_newMI'
newMI_df[7,1] = 'xgb_auc_newMI'
newMI_df[8,1] = 'bay_auc_newMI'

newMI_df[1,2] = auc_curve_over$auc
newMI_df[2,2] = auc_curve_rose$auc 
newMI_df[3,2] = svm_auc_newMI$auc
newMI_df[4,2] = nnet_singHid_auc_newMI$auc
newMI_df[5,2] = nnet_mulLogLin_auc_newMI$auc
newMI_df[6,2] = nnet_bpa_auc_newMI$auc
newMI_df[7,2] = xgb_auc_newMI$auc
newMI_df[8,2] = bay_auc_newMI$auc

newPTCA_df = data.frame(ML_algo=character(),
                  AUC=integer(),
                  stringsAsFactors=FALSE)

newPTCA_df[1,1] = 'auc_curve_over_newPTCA'
newPTCA_df[2,1] = 'auc_curve_rose_newPTCA'
newPTCA_df[3,1] = 'svm_auc_newPTCA'
newPTCA_df[4,1] = 'nnet_singHid_auc_newPTCA'
newPTCA_df[5,1] = 'nnet_mulLogLin_auc_newPTCA'
newPTCA_df[6,1] = 'nnet_bpa_auc_newPTCA'
newPTCA_df[7,1] = 'xgb_auc_newPTCA'
newPTCA_df[8,1] = 'bay_auc_newPTCA'

newPTCA_df[1,2] = auc_curve_over_newPTCA$auc
newPTCA_df[2,2] = auc_curve_rose_newPTCA$auc
newPTCA_df[3,2] = svm_auc_newPTCA$auc
newPTCA_df[4,2] = nnet_singHid_auc_newPTCA$auc
newPTCA_df[5,2] = nnet_mulLogLin_auc_newPTCA$auc
newPTCA_df[6,2] = nnet_bpa_auc_newPTCA$auc
newPTCA_df[7,2] = xgb_auc_newPTCA$auc
newPTCA_df[8,2] = bay_auc_newPTCA$auc




newCABG_df = data.frame(ML_algo=character(),
                  AUC=integer(),
                  stringsAsFactors=FALSE)

newCABG_df[1,1] = 'auc_curve_over_newCABG'
newCABG_df[2,1] = 'auc_curve_rose_newCABG'
newCABG_df[3,1] = 'svm_auc_newCABG'
newCABG_df[4,1] = 'nnet_singHid_auc_newCABG'
newCABG_df[5,1] = 'nnet_mulLogLin_auc_newCABG'
newCABG_df[6,1] = 'nnet_bpa_auc_newCABG'
newCABG_df[7,1] = 'xgb_auc_newCABG'
newCABG_df[8,1] = 'bay_auc_newCABG'

newCABG_df[1,2] = auc_curve_over_newCABG$auc
newCABG_df[2,2] = auc_curve_rose_newCABG$auc
newCABG_df[3,2] = svm_auc_newCABG$auc
newCABG_df[4,2] = nnet_singHid_auc_newCABG$auc
newCABG_df[5,2] = nnet_mulLogLin_auc_newCABG$auc
newCABG_df[6,2] = nnet_bpa_auc_newCABG$auc
newCABG_df[7,2] = xgb_auc_newCABG$auc
newCABG_df[8,2] = bay_auc_newCABG$auc


death_df = data.frame(ML_algo=character(),
                  AUC=integer(),
                  stringsAsFactors=FALSE)

death_df[1,1] = 'auc_curve_over_death'
death_df[2,1] = 'auc_curve_rose_death'
death_df[3,1] = 'svm_auc_death'
death_df[4,1] = 'nnet_singHid_auc_death'
death_df[5,1] = 'nnet_mulLogLin_auc_death'
death_df[6,1] = 'nnet_bpa_auc_death'
death_df[7,1] = 'xgb_auc_death'
death_df[8,1] = 'bay_auc_death'

death_df[1,2] = auc_curve_over_death$auc
death_df[2,2] = auc_curve_rose_death$auc
death_df[3,2] = svm_auc_death$auc
death_df[4,2] = nnet_singHid_auc_death$auc
death_df[5,2] = nnet_mulLogLin_auc_death$auc
death_df[6,2] = nnet_bpa_auc_death$auc
death_df[7,2] = xgb_auc_death$auc
death_df[8,2] = bay_auc_death$auc


newMI_best = newMI_df[which.max(newMI_df$AUC),]
newMI_best

newPTCA_best = newPTCA_df[which.max(newPTCA_df$AUC),]
newPTCA_best

newCABG_best = newCABG_df[which.max(newCABG_df$AUC),]
newCABG_best

death_best = death_df[which.max(death_df$AUC),]
death_best

shinyServer(function(input, output, session){
	output$uses <- renderUI({
                HTML("Treating heart failure caused by surgery or heart disease. It may 
                	also be used for other conditions as determined by your doctor.

					Dobutamine is an inotropic agent. It works by increasing the strength 
					and force of the heartbeat, causing more blood to circulate through the body. source: drugs.com")
	})

	output$notuse <- renderUI({
                HTML("you are allergic to any ingredient in dobutamine you have a history of heart 
                	valve problems, an adrenal gland tumor, an increased irregular heartbeat 
                	(tachyarrhythmia), or an enlarged left ventricle of the heart due to narrowing of 
                	the aortic blood vessel (idiopathic hypertrophic subaortic stenosis)
					Contact your doctor or health care provider right away if any of these apply to you. source: drugs.com")
	})

	output$beforeuse <- renderUI({
                HTML("Some medical conditions may interact with dobutamine. Tell your doctor or pharmacist 
                	if you have any medical conditions, especially if any of the following apply to you:

					if you are pregnant, planning to become pregnant, or are breast-feeding
					if you are taking any prescription or nonprescription medicine, herbal preparation, or 
					dietary supplement
					if you have allergies to medicines, foods, or other substances
					if you have an irregular heartbeat, low blood volume levels, diabetes, or high blood 
					pressure
					Some MEDICINES MAY INTERACT with dobutamine. Tell your health care provider if you are 
					taking any other medicines, especially any of the following:

					Cimetidine or methyldopa because side effects of dobutamine may be increased
					Catechol-O-methyltransferase (COMT) inhibitors (eg, entacapone) or droxidopa 
					because actions and side effects may be increased by dobutamine
					This may not be a complete list of all interactions that may occur. Ask your health 
					care provider if dobutamine may interact with other medicines that you take. Check with 
					your health care provider before you start, stop, or change the dose of any medicine. source: drugs.com")
	})

	output$howtouse <- renderUI({
                HTML("Use dobutamine as directed by your doctor. Check the label on the medicine for exact 
                	dosing instructions.

					Dobutamine is administered as an injection at your doctor's office, hospital, or clinic.
					If dobutamine contains particles or is discolored, or if the vial is cracked or damaged 
					in any way, do not use it.
					Keep this product, as well as syringes and needles, out of the reach of children and away 
					from pets. Do not reuse needles, syringes, or other materials. Dispose of properly after 
					use. Ask your doctor or pharmacist to explain local regulations for proper disposal.


					If you miss a dose of dobutamine, contact your doctor immediately.
					Ask your health care provider any questions you may have about how to use dobutamine. source: drugs.com")
	 })

	 output$safety <- renderUI({
                HTML("Dobutamine may turn pink after exposure to light. This is normal and not a cause 
                	for concern. Some of these products contain sulfites, which can cause allergic reactions 
					in certain individuals (eg, asthma patients). If you have previously had allergic 
					reactions to sulfites, contact your pharmacist to determine if the product you are 
					taking contains sulfites.
					Before you have any medical or dental treatments, emergency care, or surgery, tell the 
					doctor or dentist that you are using dobutamine.
					LAB TESTS, including blood potassium levels, blood pressure, and heart rate, may be 
					performed to monitor your progress or to check for side effects. Be sure to keep all 
					doctor and lab appointments.
					PREGNANCY and BREAST-FEEDING: If you become pregnant, discuss with your doctor the 
					benefits and risks of using dobutamine during pregnancy. If you are or will be 
					breast-feeding while you are using dobutamine, check with your doctor or pharmacist 
					to discuss the risks to your baby. source: drugs.com")
	})

	output$sideeffect <- renderUI({
                HTML("All medicines may cause side effects, but many people have no, or minor, side 
                	effects. Check with your doctor if any of these most COMMON side effects persist or 
                	become bothersome. If you have any questions about dobutamine, please talk with your 
                	doctor, pharmacist, or other health care provider.
					Dobutamine is to be used only by the patient for whom it is prescribed. Do not share 
					it with other people.
					If your symptoms do not improve or if they become worse, check with your doctor.
					Check with your pharmacist about how to dispose of unused medicine.
					This information should not be used to decide whether or not to take dobutamine or any
					other medicine. Only your health care provider has the knowledge and training to decide
					which medicines are right for you. This information does not endorse any medicine as
					safe, effective, or approved for treating any patient or health condition.
					This is only a brief summary of general information about dobutamine. It does 
					NOT include all information about the possible uses, directions, warnings, 
					precautions, interactions, adverse effects, or risks that may apply to dobutamine.
					This information is not specific medical advice and does not replace information 
					you receive from your health care provider. You must talk with your healthcare
					provider for complete information about the risks and benefits of using dobutamine. source: drugs.com")
	 })
		
	output$echocardiogram <- renderUI({
                HTML("An echocardiogram (echo) is a test used to assess the heart's function and structures.
                	A stress echocardiogram is a test done to assess how well the heart works under stress. 
                	The “stress” can be triggered by either exercise on a treadmill or medication called dobutamine.

					A dobutamine stress echocardiogram (DSE) may be used if you are unable to exercise. 
					Dobutamine is put in a vein and causes the heart to beat faster. It mimics the effects 
					of exercise on the heart.

					During an echo, a transducer (like a microphone) sends out ultrasonic sound waves at a frequency 
					too high to be heard. When the transducer is placed on the chest at certain locations and angles, 
					the ultrasonic sound waves move through the skin and other body tissues to the heart tissues, where 
					the waves bounce or echo off of the heart structures. The transducer picks up the reflected waves 
					and sends them to a computer. The computer displays the echoes as images of the heart walls and valves. source: John Hopkins Medicine")
	})

     output$plotnewMI = renderPlot({
         plot(tsne_newMI$Y, t = 'n', main = "tsne_newMI")
         text(tsne_newMI$Y, label = as.factor(train$newMI), col = colors_newMI[as.factor(train$newMI)])
    })
     
    output$plotnewPTCA = renderPlot({
         plot(tsne_newPTCA$Y, t = 'n', main = "tsne_newPTCA")
         text(tsne_newPTCA$Y, label = as.factor(train$newPTCA), col = colors_newPTCA[as.factor(train$newPTCA)])
    })
     
     output$plotnewCABG = renderPlot({
         plot(tsne_newCABG$Y, t = 'n', main = "tsne_newCABG")
         text(tsne_newCABG$Y, label = as.factor(train$newCABG), col = colors_newCABG[as.factor(train$newCABG)])
     })
     
     output$plotdeath = renderPlot({
         plot(tsne_death$Y, t = 'n', main = "tsne_death")
         text(tsne_death$Y, label = as.factor(train$death), col = colors_death[as.factor(train$death)])
     })
     
     output$plottree_newMI = renderPlot({
         fancyRpartPlot(tree_rose)
     })
     
     output$plottree_newPTCA = renderPlot({
         fancyRpartPlot(tree_rose_newPTCA)
     })
     
     output$plottree_newCABG = renderPlot({
         fancyRpartPlot(tree_rose_newCABG)
     })
     
     output$plottree_death = renderPlot({
         fancyRpartPlot(tree_rose_death)
     })	

     ##################################################################################################################
     output$plottreeauc_newMI = renderPlot({
         roc.curve(test$newMI, pred_rose)
     })
     
     output$plotsvmauc_newMI = renderPlot({
         roc.curve(svm_test$newMI, Predicted_y_proper_newMI)
     })
     
     output$plotnnet_singHidauc_newMI = renderPlot({
         roc.curve(nnet_test$newMI, nnet_singHid_pred_newMI)
     })	
     
     output$plotnnet_mulLogLinauc_newMI = renderPlot({
         roc.curve(nnet_test$newMI, nnet_mulLogLin_pred_newMI)
     })
     
     output$plotnnet_bpaauc_newMI = renderPlot({
         roc.curve(nnet_test$newMI, nnet_bpa_pred_newMI$net.result)
     })
     
     output$plotxgbauc_newMI = renderPlot({
         roc.curve(xgb_y_test_newMI, xgb_predict_newMI)
     })
     
     output$plotnaivebayesauc_newMI = renderPlot({
         roc.curve(bay_test$newMI, bay_pred_newMI)
     })
     
     ##################################################################################################################						
     output$plottreeauc_newPTCA = renderPlot({
         roc.curve(test$newPTCA, pred_rose_newPTCA)
     })
     
     output$plotsvmauc_newPTCA = renderPlot({
         roc.curve(svm_test$newPTCA, Predicted_y_proper_newPTCA)
     })
     
     output$plotnnet_singHidauc_newPTCA = renderPlot({
         roc.curve(nnet_test$newPTCA, nnet_singHid_pred_newPTCA)
     })	
     
     output$plotnnet_mulLogLinauc_newPTCA = renderPlot({
         roc.curve(nnet_test$newPTCA, nnet_mulLogLin_pred_newPTCA)
     })
     
     output$plotnnet_bpaauc_newPTCA = renderPlot({
         roc.curve(nnet_test$newPTCA, nnet_bpa_pred_newPTCA$net.result)
     })
     
     output$plotxgbauc_newPTCA = renderPlot({
         roc.curve(xgb_y_test_newPTCA, xgb_predict_newPTCA)
     })
     
     output$plotnaivebayesauc_newPTCA = renderPlot({
         roc.curve(bay_test$newPTCA, bay_pred_newPTCA)
     })					
     
     ##################################################################################################################						
     output$plottreeauc_newCABG = renderPlot({
         roc.curve(test$newCABG, pred_rose_newCABG)
     })
     
     output$plotsvmauc_newCABG = renderPlot({
         roc.curve(svm_test$newCABG, Predicted_y_proper_newCABG)
     })
     
     output$plotnnet_singHidauc_newCABG = renderPlot({
         roc.curve(nnet_test$newCABG, nnet_singHid_pred_newCABG)
     })	
     
     output$plotnnet_mulLogLinauc_newCABG = renderPlot({
         roc.curve(nnet_test$newCABG, nnet_mulLogLin_pred_newCABG)
     })
     
     output$plotnnet_bpaauc_newCABG = renderPlot({
         roc.curve(nnet_test$newCABG, nnet_bpa_pred_newCABG$net.result)
     })
     
     output$plotxgbauc_newCABG = renderPlot({
         roc.curve(xgb_y_test_newCABG, xgb_predict_newCABG)
     })
     
     output$plotnaivebayesauc_newCABG = renderPlot({
         roc.curve(bay_test$newCABG, bay_pred_newCABG)
     })			
      
     ##################################################################################################################						
     output$plottreeauc_death = renderPlot({
         roc.curve(test$death, pred_rose_death)
     })
     
     output$plotsvmauc_death = renderPlot({
         roc.curve(svm_test$death, Predicted_y_proper_death)
     })
     
     output$plotnnet_singHidauc_death = renderPlot({
         roc.curve(nnet_test$death, nnet_singHid_pred_death)
     })	
     
     output$plotnnet_mulLogLinauc_death = renderPlot({
         roc.curve(nnet_test$death, nnet_mulLogLin_pred_death)
     })
     
     output$plotnnet_bpaauc_death = renderPlot({
         roc.curve(nnet_test$death, nnet_bpa_pred_death$net.result)
     })
     
     output$plotxgbauc_death = renderPlot({
         roc.curve(xgb_y_test_death, xgb_predict_death)
     })
     
     output$plotnaivebayesauc_death = renderPlot({
         roc.curve(bay_test$death, bay_pred_death)
     })
     
     ##################################################################################################################								
     output$newMI_chart = renderChart2({
         newMI_PLOT <- hPlot(AUC ~ ML_algo, data = newMI_df, type = 'column', group.na = 'NA\'s')
         newMI_PLOT$title(text = "Best Algorithm for ClASSIFYING newMI Dependent Variable")
         newMI_PLOT$subtitle(text = paste0("For classification of those who are most likely to have cardiac arrest, we use the: ", toupper(newMI_best[1]), ".", " It produced an AUC of: 		", newMI_best[2]))
         newMI_PLOT$plotOptions(column = list(dataLabels = list(enabled = T, rotation = -90, align = 'right', color = 'purple', x = 4, y = 10, style = list(fontSize = '13px', fontFamily 		= 'Verdana, sans-serif'))))
         newMI_PLOT$xAxis(labels = list(rotation = -90, align = 'right', style = list(fontSize = '13px', fontFamily = 'Verdana, sans-serif')), replace = F)
         return(newMI_PLOT)
     })
     
     output$newPTCA_chart = renderChart2({
         newPTCA_PLOT <- hPlot(AUC ~ ML_algo, data = newPTCA_df, type = 'column', group.na = 'NA\'s')
         newPTCA_PLOT$title(text = "Best Algorithm for ClASSIFYING newPTCA Dependent Variable")
         newPTCA_PLOT$subtitle(text = paste0("For classification of those who are most likely to have PTCA, we use the: ", toupper(newPTCA_best[1]), ".", " It produced an AUC of: ", 			newPTCA_best[2]))
         newPTCA_PLOT$plotOptions(column = list(dataLabels = list(enabled = T, rotation = -90, align = 'right', color = 'purple', x = 4, y = 10, style = list(fontSize = '13px', 				fontFamily = 'Verdana, sans-serif'))))
         newPTCA_PLOT$xAxis(labels = list(rotation = -90, align = 'right', style = list(fontSize = '13px', fontFamily = 'Verdana, sans-serif')), replace = F)
         return(newPTCA_PLOT)
     })
     
     output$newCABG_chart = renderChart2({
         newCABG_PLOT <- hPlot(AUC ~ ML_algo, data = newCABG_df, type = 'column', group.na = 'NA\'s')
         newCABG_PLOT$title(text = "Best Algorithm for ClASSIFYING newCABG Dependent Variable")
         newCABG_PLOT$subtitle(text = paste0("For classification of those who are most likely to have CABG, we use the: ", toupper(newCABG_best[1]), ".", " It produced an AUC of: ", 			newCABG_best[2]))
         newCABG_PLOT$plotOptions(column = list(dataLabels = list(enabled = T, rotation = -90, align = 'right', color = 'purple', x = 4, y = 10, style = list(fontSize = '13px', 				fontFamily = 'Verdana, sans-serif'))))
         newCABG_PLOT$xAxis(labels = list(rotation = -90, align = 'right', style = list(fontSize = '13px', fontFamily = 'Verdana, sans-serif')), replace = F)
         newCABG_PLOT
     })
     
     output$death_chart = renderChart2({
         death_PLOT <- hPlot(AUC ~ ML_algo, data = death_df, type = 'column', group.na = 'NA\'s')
         death_PLOT$title(text = "Best Algorithm for ClASSIFYING death Dependent Variable")
         death_PLOT$subtitle(text = paste0("For classification of those who are most likely to die, we use the: ", toupper(death_best[1]), ".", " It produced an AUC of: ", 						death_best[2]))
         death_PLOT$plotOptions(column = list(dataLabels = list(enabled = T, rotation = -90, align = 'right', color = 'purple', x = 4, y = 10, style = list(fontSize = '13px', fontFamily 		= 'Verdana, sans-serif'))))
         death_PLOT$xAxis(labels = list(rotation = -90, align = 'right', style = list(fontSize = '13px', fontFamily = 'Verdana, sans-serif')), replace = F)
         return(death_PLOT)
     })
	}
)

